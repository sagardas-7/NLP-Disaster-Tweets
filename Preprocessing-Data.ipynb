{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93934a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/imerit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/imerit/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/imerit/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/imerit/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b370ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f02d911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a53887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "damage                   41\n",
       "body%20bags              41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dfcacf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = df[[\"text\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faa40b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this earthquake May ALLAH Forgive us all\n"
     ]
    }
   ],
   "source": [
    "#remove all hashtags\n",
    "text = \"\"\"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\"\"\n",
    "def remove_hashtags(text):\n",
    "    text = text.replace('#','')\n",
    "    return text\n",
    "\n",
    "print(remove_hashtags(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20db4cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the 1960s the oryx a symbol of the Arabian Peninsula were annihilated by hunters. \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#remove all brackets and links\n",
    "text = \"\"\"During the 1960s the oryx a symbol of the [Arabian Peninsula] were annihilated by hunters. \\nhttp://t.co/yangEQBUQW http://t.co/jQ2eH5KGLt \"\"\"\n",
    "def remove_brackets_links(text):\n",
    "    text = re.sub(r'[\\([{})\\]]', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return(text)\n",
    "\n",
    "print(remove_brackets_links(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1f9093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " During the 1960s the oryx a symbol of the [Arabian Peninsula] were annihilated by hunters. http://t.co/yangEQBUQW http://t.co/jQ2eH5KGLt\n"
     ]
    }
   ],
   "source": [
    "# Remove all the newlines('\\n'), tabs('\\t'), \"-\", \"\\\".   and word end with :\n",
    "text = \"\"\"During the 1960s the or-yx a symbol of the [Arabian Peninsula] were annihilated by hunters. \\nhttp://t.co/yangEQBUQW http://t.co/jQ2eH5KGLt \"\"\"\n",
    "def  remove_escape(text):\n",
    "    all_lines = [line for line in text.split('\\n')]\n",
    "    new_file = ''\n",
    "\n",
    "\n",
    "    for each_line in all_lines:\n",
    "        new_line = \"\"\n",
    "        for element in each_line:\n",
    "            if element == '\\t' or element == '-' or element == '':\n",
    "                continue\n",
    "            new_line = new_line + element\n",
    "        new_file+=new_line\n",
    "\n",
    "    final_file=''\n",
    "    words = new_file.split()\n",
    "    for each_word in words:\n",
    "\n",
    "        if each_word.isdigit():\n",
    "            continue\n",
    "        if each_word[-1]==':':\n",
    "            continue\n",
    "            \n",
    "        final_file= final_file + \" \" +each_word\n",
    "\n",
    "    text = final_file\n",
    "    return text\n",
    "\n",
    "print(remove_escape(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0539bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31443fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e61b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arabian Peninsula']\n"
     ]
    }
   ],
   "source": [
    ">>> from nltk import ne_chunk, pos_tag, word_tokenize\n",
    ">>> from nltk.tree import Tree\n",
    ">>> \n",
    ">>> def get_continuous_chunks(text):\n",
    "...     chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "...     continuous_chunk = []\n",
    "...     current_chunk = []\n",
    "...     for i in chunked:\n",
    "...             if type(i) == Tree:\n",
    "...                     current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "...             if current_chunk:\n",
    "...                     named_entity = \" \".join(current_chunk)\n",
    "...                     if named_entity not in continuous_chunk:\n",
    "...                             continuous_chunk.append(named_entity)\n",
    "...                             current_chunk = []\n",
    "...             else:\n",
    "...                     continue\n",
    "...     return continuous_chunk\n",
    "\n",
    "print(get_continuous_chunks(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ffa924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the 1960s the or-yx a symbol of the [] were annihilated by hunters. \n",
      "http://t.co/yangEQBUQW http://t.co/jQ2eH5KGLt \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Text chunking, also referred to as shallow parsing, is a task that \n",
    "follows Part-Of-Speech Tagging and that adds more structure to the sentence.\n",
    "So it combines the some phrases, named entities into single word.\n",
    "So after that combine all those phrases/named entities by separating \"_\". \n",
    "And remove the phrases/named entities if that is a \"Person\". \n",
    "You can use nltk.ne_chunk to get these. \n",
    "Below we have given one example. please go through it. \n",
    "\n",
    "useful links: \n",
    "https://www.nltk.org/book/ch07.html\n",
    "https://stackoverflow.com/a/31837224/4084039\n",
    "http://www.nltk.org/howto/tree.html\n",
    "https://stackoverflow.com/a/44294377/4084039\n",
    "\n",
    "\"\"\"\n",
    "def chunking(text):\n",
    "    label_list = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                label_list.append(chunk.label())\n",
    "\n",
    "\n",
    "    my_chunks = get_continuous_chunks(text)\n",
    "\n",
    "    #print(label_list)\n",
    "    #print(my_chunks)\n",
    "    i=0\n",
    "    for each1 in my_chunks:\n",
    "        #print(i)\n",
    "        #print(each1)\n",
    "        #print(label_list)\n",
    "        #print(my_chunks)\n",
    "        if i >= len(label_list):\n",
    "            break\n",
    "        if label_list[i] == 'PERSON':\n",
    "            text = text.replace(each1,'')\n",
    "        if label_list[i] == 'GPE' and len(my_chunks[i].split())>1 :\n",
    "            new_chunk = ''\n",
    "            for each in my_chunks[i].split():\n",
    "                new_chunk = new_chunk+each+\"_\"\n",
    "            new_chunk = new_chunk[:-1]\n",
    "            #print(new_chunk)\n",
    "            text = text.replace(each1,new_chunk)\n",
    "        #print(\"####################################\")\n",
    "        i+=1\n",
    "\n",
    "    return text\n",
    "print(chunking(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03562eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello there'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def keep_only_alphnumeric(text):\n",
    "    line = ''\n",
    "    for each in text.split():\n",
    "        #print(each)\n",
    "        words = re.sub(r'\\W+', '', each)\n",
    "        line = line + words + ' ' \n",
    "    #print(line)\n",
    "    line = line[:-1]\n",
    "    return line\n",
    "\n",
    "keep_only_alphnumeric('Hello there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e90d4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extras(text):\n",
    "    all_lines = [line for line in text.split('\\n')]\n",
    "    new_file = ''\n",
    "\n",
    "    for each_line in all_lines:\n",
    "        new_line = \"\"\n",
    "        for element in each_line:\n",
    "            new_line = new_line + element\n",
    "        new_file+=new_line\n",
    "\n",
    "\n",
    "    final_file=''\n",
    "    words = new_file.split()\n",
    "#     print(words)\n",
    "    for each_word in words:\n",
    "        if(len(each_word)<=2 or len(each_word)>=15):\n",
    "            continue\n",
    "        if each_word.isdigit():\n",
    "            continue\n",
    "        if each_word[0]==\"_\" and each_word[-1]=='_':\n",
    "            each_word = each_word[1:-1]\n",
    "        if each_word[0]=='_':\n",
    "            each_word = each_word[1:]\n",
    "        if each_word[-1]=='_':\n",
    "            each_word = each_word[:-1]\n",
    "        pos = each_word.find('_')\n",
    "        if pos <= 2 and pos != -1:\n",
    "            each_word = each_word[pos+1:]\n",
    "        if(len(each_word)<=2 or len(each_word)>=15):\n",
    "            continue\n",
    "        each_word = each_word.lower()\n",
    "        \n",
    "\n",
    "        if each_word.isalpha() == False:\n",
    "            continue\n",
    "            \n",
    "        final_file= final_file +each_word + \" \"\n",
    "    return final_file[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c8c249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_array):\n",
    "    text_array = list(text_array)\n",
    "    #print(text_array)\n",
    "    new_text_array = []\n",
    "    i = 1\n",
    "    for each_text in text_array:\n",
    "        each_text = each_text[0]\n",
    "        each_text = each_text.lower()\n",
    "        each_text = remove_hashtags(each_text)\n",
    "        each_text = remove_brackets_links(each_text)\n",
    "        each_text = remove_escape(each_text)\n",
    "        each_text = decontracted(each_text)\n",
    "        each_text = chunking(each_text)\n",
    "        each_text = keep_only_alphnumeric(each_text) \n",
    "        each_text = remove_extras(each_text)   \n",
    "        new_text_array.append(each_text)\n",
    "        #print(new_text_array)\n",
    "    return new_text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0d6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = preprocess_text(text_array)\n",
    "df['new_texts'] = new_texts\n",
    "df = df.drop('text', 1)\n",
    "df = df[['keyword', 'location', 'new_texts', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "612ccbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"text_preprocessed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
